# -*- coding: utf-8 -*-
"""Submission1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SiQLVYWvrLGZW3mcr5MWd4-5bZUha31_

*   Nama: Tuti Amalia
*   Email: amalia.tuti111@gmail.com
*   Asal: Kabupaten Gowa
*   Proyek pertama: menggunakan model NLP
*   Dataset yang digunakan: https://www.kaggle.com/shauryavardhan1/emotion-classifier-dataset

Library
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from tensorflow.keras.models import Sequential

"""Download Dataset"""

!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=12ljMpZL6iYxdiiFSxPcZMbbKh_kixfex'  \
  -O 'archive (1).zip'

# melakukan ekstraksi pada file zip
import zipfile,os
fileZip = 'archive (1).zip'
zipRef = zipfile.ZipFile(fileZip, 'r')
zipRef.extractall('')
zipRef.close()

df = pd.read_csv('Emotion.csv', encoding='windows-1252')
# encoding='utf-8'
df.head()

"""Mengambil"""

df = df.drop(['id', 'confidence'], axis=1)
df.head()

"""Jumlah sampel yang digunakan"""

len(df)

plt.figure(figsize=(10,5))
sns.countplot(df['Emotion'])
plt.xticks(rotation=45)
plt.show()

df['Emotion'].value_counts()

#menghapus data yang duplikat
df = df.drop_duplicates(subset="Text")
df.info()

"""Pembersihan data"""

from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
def clean_text(text):
    """
        text: a string 
        return: modified clean string
    """
    result = ""
    for token in simple_preprocess(text):
        if token not in STOPWORDS and len(token) >= 2:
            token = token.lower() 
            result+=token+" "     
    return result

df['Text'] = df['Text'].map(clean_text)
df.head()

"""Oversampling data yang tidak seimbang"""

X = df.drop('Emotion',axis=1)
y = df['Emotion']

from imblearn.over_sampling import RandomOverSampler
oversampler = RandomOverSampler(random_state=10)
X_resample, y_resample = oversampler.fit_resample(X,y)

# Check the Fraud and Genuine data distribution after over sampling
from collections import Counter
print('Actual dataset {}'.format(Counter(y)))
print('Undersampled dataset {}'.format(Counter(y_resample)))

y_resample = pd.DataFrame(y_resample)
y_resample.value_counts()

"""Ploting hasil oversampling data"""

plt.figure(figsize=(10,5))
sns.countplot(y_resample[0])
plt.xticks(rotation=45)
plt.show()

X_resample = pd.DataFrame(X_resample)
X_resample

y_resample

"""Mengubah Emotion menjadi One Hot Vector"""

review = pd.get_dummies(y_resample[0])
review.head()

X_resample = X_resample.rename(columns={0 : 'Text'})
X_resample

"""Menggabungkan Emotion dengan text dimana sudah dioversampling"""

new_df = pd.concat([X_resample, review], axis=1)
new_df

new_df.shape

X = new_df['Text'].values
y = new_df[['anger', 'fear', 'joy', 'sadness']].values
print(X)
print(y)

"""Membagi validation set sebesar 20% dari total dataset"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)
print("Total data train: ", len(y_train))
print("Total data test: ", len(y_test))

"""Menggunakan fungsi tokenizer"""

tokenizer = Tokenizer(num_words=10000, oov_token='x')

tokenizer.fit_on_texts(X_train)
tokenizer.fit_on_texts(X_test)

sequence_train = tokenizer.texts_to_sequences(X_train)
sequence_test = tokenizer.texts_to_sequences(X_test)

padded_train = pad_sequences(sequence_train, maxlen=300, padding="post", truncating="post")
padded_test = pad_sequences(sequence_test, maxlen=300, padding="post", truncating="post")

"""Menggunakan model sequential dan layer Embedding"""

model = Sequential([
                    tf.keras.layers.Embedding(10000, output_dim=300),
                    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
                    tf.keras.layers.Dropout(0.5),
                    tf.keras.layers.Flatten(),
                    tf.keras.layers.Dense(128, activation='relu'),
                    tf.keras.layers.Dense(64, activation='relu'),
                    tf.keras.layers.Dropout(0.5),
                    tf.keras.layers.Dense(4, activation='softmax')
])

model.summary()

# Compile model
model.compile(tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

print('Compiling Model.......')

"""Menggunakan Callback"""

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('val_accuracy') > 0.90):
      print('\nYeay, akurasi memenuhi target!')
      self.model.stop_training = True

callbacks = myCallback()

"""Menjalankan Model"""

history = model.fit(padded_train, 
                    y_train, 
                    epochs=30, 
                    validation_data=(padded_test, y_test), 
                    callbacks=[callbacks], 
                    verbose=2)

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.ylabel('value')
plt.xlabel('No. epoch')
plt.legend(loc="upper left")
plt.show()

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.ylabel('value')
plt.xlabel('No. epoch')
plt.legend(loc="upper left")
plt.show()

